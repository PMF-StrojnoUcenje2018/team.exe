{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primjer treniranja CNN-a na training setu velicine 261 i testiranje na balansiranom setu veliƒçine 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Josip\\mfsc_cnn_fun.py:309: RuntimeWarning: divide by zero encountered in log10\n",
      "  ims = 20.*np.log10(np.abs(sshow)/10e-6) # amplitude to decibel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal frames test:296\n",
      "Abnormal frames test:325\n",
      "Normal frames validation:74\n",
      "Abnormal frames validation:71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Josip\\mfsc_cnn_fun.py:326: RuntimeWarning: divide by zero encountered in log10\n",
      "  ims = 20.*np.log10(np.abs(sshow)/10e-6) # amplitude to decibel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "592 train sequences\n",
      "142 test sequences\n",
      "X_train shape: (592, 3, 129, 129)\n",
      "X_test shape: (142, 3, 129, 129)\n",
      "Y_train shape: (592, 2)\n",
      "Y_test shape: (142, 2)\n",
      "Build model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Josip\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:65: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(filters=90, padding=\"valid\", input_shape=(3, 129, 1..., data_format=\"channels_first\", kernel_size=(3, 57), kernel_initializer=\"uniform\", kernel_regularizer=<keras.reg..., kernel_constraint=<keras.con...)`\n",
      "C:\\Users\\Josip\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:80: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(90, (1, 3), kernel_regularizer=<keras.reg..., kernel_initializer=\"uniform\", data_format=\"channels_first\", kernel_constraint=<keras.con...)`\n",
      "C:\\Users\\Josip\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:97: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(256, kernel_regularizer=<keras.reg..., kernel_initializer=\"uniform\", kernel_constraint=<keras.con...)`\n",
      "C:\\Users\\Josip\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:104: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(256, kernel_regularizer=<keras.reg..., kernel_initializer=\"uniform\", kernel_constraint=<keras.con...)`\n",
      "C:\\Users\\Josip\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:127: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 592 samples, validate on 142 samples\n",
      "Epoch 1/50\n",
      "592/592 [==============================] - 6s 10ms/step - loss: 0.9151 - acc: 0.5659 - val_loss: 0.7452 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.74521, saving model to Model1_weights.hdf5\n",
      "Epoch 2/50\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.7481 - acc: 0.6233 - val_loss: 0.6838 - val_acc: 0.8028\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.74521 to 0.68382, saving model to Model1_weights.hdf5\n",
      "Epoch 3/50\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.6919 - acc: 0.6453 - val_loss: 0.6942 - val_acc: 0.8239\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.68382\n",
      "Epoch 4/50\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.6789 - acc: 0.6318 - val_loss: 0.6903 - val_acc: 0.8169\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.68382\n",
      "Epoch 5/50\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.6794 - acc: 0.6486 - val_loss: 0.6886 - val_acc: 0.8239\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.68382\n",
      "Epoch 6/50\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.6443 - acc: 0.6774 - val_loss: 0.6431 - val_acc: 0.6901\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.68382 to 0.64309, saving model to Model1_weights.hdf5\n",
      "Epoch 7/50\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.6968 - acc: 0.6030 - val_loss: 0.7191 - val_acc: 0.5211\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.64309\n",
      "Epoch 8/50\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.6804 - acc: 0.6351 - val_loss: 0.7121 - val_acc: 0.5282\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.64309\n",
      "Epoch 9/50\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.6488 - acc: 0.6571 - val_loss: 0.6512 - val_acc: 0.8099\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.64309\n",
      "Epoch 10/50\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.6394 - acc: 0.6959 - val_loss: 0.6960 - val_acc: 0.5352\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.64309\n",
      "Epoch 11/50\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.6240 - acc: 0.6959 - val_loss: 0.5982 - val_acc: 0.7958\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.64309 to 0.59822, saving model to Model1_weights.hdf5\n",
      "Epoch 12/50\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.6215 - acc: 0.6892 - val_loss: 0.6548 - val_acc: 0.6901\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.59822\n",
      "Epoch 13/50\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.6092 - acc: 0.7095 - val_loss: 0.6393 - val_acc: 0.7535\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.59822\n",
      "Epoch 14/50\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.5854 - acc: 0.7264 - val_loss: 0.6782 - val_acc: 0.7113\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.59822\n",
      "Epoch 15/50\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.5715 - acc: 0.7382 - val_loss: 0.6195 - val_acc: 0.7746\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.59822\n",
      "Epoch 16/50\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.5846 - acc: 0.7314 - val_loss: 0.6858 - val_acc: 0.5986\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.59822\n",
      "Epoch 17/50\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.5514 - acc: 0.7703 - val_loss: 0.6090 - val_acc: 0.6972\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.59822\n",
      "Epoch 18/50\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.5254 - acc: 0.7652 - val_loss: 0.7059 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.59822\n",
      "Epoch 19/50\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.5236 - acc: 0.7720 - val_loss: 0.6568 - val_acc: 0.6549\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.59822\n",
      "Epoch 20/50\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.5140 - acc: 0.7973 - val_loss: 0.6209 - val_acc: 0.7535\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.59822\n",
      "Epoch 21/50\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.4821 - acc: 0.8108 - val_loss: 0.6496 - val_acc: 0.6901\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.59822\n",
      "Epoch 22/50\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.4809 - acc: 0.7990 - val_loss: 0.5741 - val_acc: 0.7254\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.59822 to 0.57409, saving model to Model1_weights.hdf5\n",
      "Epoch 23/50\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.4235 - acc: 0.8378 - val_loss: 0.5343 - val_acc: 0.7535\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.57409 to 0.53426, saving model to Model1_weights.hdf5\n",
      "Epoch 24/50\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.3949 - acc: 0.8615 - val_loss: 0.5540 - val_acc: 0.7465\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.53426\n",
      "Epoch 25/50\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.3791 - acc: 0.8564 - val_loss: 0.5547 - val_acc: 0.7394\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.53426\n",
      "Epoch 26/50\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.3577 - acc: 0.8733 - val_loss: 0.5667 - val_acc: 0.6901\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.53426\n",
      "Epoch 27/50\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.3648 - acc: 0.8615 - val_loss: 0.4998 - val_acc: 0.7606\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.53426 to 0.49982, saving model to Model1_weights.hdf5\n",
      "Epoch 28/50\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.3149 - acc: 0.8868 - val_loss: 0.5117 - val_acc: 0.7394\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.49982\n",
      "Epoch 29/50\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.3118 - acc: 0.9003 - val_loss: 0.5086 - val_acc: 0.7887\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.49982\n",
      "Epoch 30/50\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.2926 - acc: 0.9003 - val_loss: 0.5108 - val_acc: 0.7535\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.49982\n",
      "Epoch 31/50\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.2567 - acc: 0.9223 - val_loss: 0.5176 - val_acc: 0.7042\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.49982\n",
      "Epoch 32/50\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.3133 - acc: 0.8801 - val_loss: 0.5047 - val_acc: 0.7113\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.49982\n",
      "Epoch 33/50\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.2476 - acc: 0.9240 - val_loss: 0.4319 - val_acc: 0.7606\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.49982 to 0.43190, saving model to Model1_weights.hdf5\n",
      "Epoch 34/50\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.2361 - acc: 0.9240 - val_loss: 0.4775 - val_acc: 0.7606\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.43190\n",
      "Epoch 35/50\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.2509 - acc: 0.9291 - val_loss: 0.4933 - val_acc: 0.7465\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.43190\n",
      "Epoch 36/50\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.2254 - acc: 0.9274 - val_loss: 0.4262 - val_acc: 0.8028\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.43190 to 0.42616, saving model to Model1_weights.hdf5\n",
      "Epoch 37/50\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.2174 - acc: 0.9375 - val_loss: 0.4679 - val_acc: 0.7535\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.42616\n",
      "Epoch 38/50\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.1896 - acc: 0.9527 - val_loss: 0.4701 - val_acc: 0.7746\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.42616\n",
      "Epoch 39/50\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.1988 - acc: 0.9426 - val_loss: 0.4928 - val_acc: 0.7254\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.42616\n",
      "Epoch 40/50\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.1716 - acc: 0.9527 - val_loss: 0.4863 - val_acc: 0.7676\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.42616\n",
      "Epoch 41/50\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.1654 - acc: 0.9611 - val_loss: 0.4494 - val_acc: 0.7606\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.42616\n",
      "Epoch 42/50\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.1498 - acc: 0.9679 - val_loss: 0.4073 - val_acc: 0.7817\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.42616 to 0.40731, saving model to Model1_weights.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/50\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.1358 - acc: 0.9645 - val_loss: 0.4845 - val_acc: 0.7394\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.40731\n",
      "Epoch 44/50\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.1570 - acc: 0.9561 - val_loss: 0.4504 - val_acc: 0.7958\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.40731\n",
      "Epoch 45/50\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.1247 - acc: 0.9865 - val_loss: 0.4089 - val_acc: 0.7958\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.40731\n",
      "Epoch 46/50\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.1156 - acc: 0.9747 - val_loss: 0.4178 - val_acc: 0.7817\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.40731\n",
      "Epoch 47/50\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.1146 - acc: 0.9780 - val_loss: 0.4710 - val_acc: 0.7254\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.40731\n",
      "Epoch 48/50\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.1237 - acc: 0.9730 - val_loss: 0.3820 - val_acc: 0.8028\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.40731 to 0.38197, saving model to Model1_weights.hdf5\n",
      "Epoch 49/50\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.1244 - acc: 0.9780 - val_loss: 0.4972 - val_acc: 0.7465\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.38197\n",
      "Epoch 50/50\n",
      "592/592 [==============================] - 4s 7ms/step - loss: 0.1185 - acc: 0.9730 - val_loss: 0.4479 - val_acc: 0.7958\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.38197\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.constraints import maxnorm\n",
    "import mfsc_cnn_fun as dataimport\n",
    "from keras.models import model_from_json\n",
    "\n",
    "\n",
    "\n",
    "max_features = 5000\n",
    "maxlen = 100\n",
    "batch_size = 32\n",
    "embedding_dims = 100\n",
    "nb_filter = 90\n",
    "hidden_dims = 256\n",
    "nb_epoch = 50\n",
    "nb_classes = 2\n",
    "optimizer=Adam(lr=0.0001,decay=1e-6)\n",
    "loss = 'categorical_crossentropy'\n",
    "model_json='hb_model_orthogonal_experiment_norm.json'\n",
    "weights='Model1_weights.hdf5'\n",
    "seed = 1995\n",
    "test_split = 0.2\n",
    "normal_path ='C:\\\\Users\\\\Josip\\\\test set\\\\normal\\\\'\n",
    "abnormal_path ='C:\\\\Users\\\\Josip\\\\test set\\\\abnormal\\\\'\n",
    "\n",
    "\n",
    "print('Loading data...')\n",
    "#Loads training data from specified folders for normal and abnormal sound files. \n",
    "#Transfrorms data using short-time Fourier transform, logscales the result and splits it into 129x129 squares.\n",
    "#Randomizes data.\n",
    "#Splits the data into train and test arrays\n",
    "(X_train, y_train), (X_test, y_test) = dataimport.load_data(normal_path=normal_path, abnormal_path=abnormal_path, test_split=test_split, width=129, height=256, seed=seed)\n",
    "\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "print('Y_train shape:', Y_train.shape)\n",
    "print('Y_test shape:', Y_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "# We start off with using Convolution2D for a frame\n",
    "# The filter is 3x57\n",
    "model.add(Convolution2D(nb_filter=nb_filter,\n",
    "                        nb_row=3,\n",
    "                        nb_col=57,\n",
    "                        init='uniform',\n",
    "                        border_mode='valid',\n",
    "                        W_regularizer=l2(0.0001),\n",
    "                        W_constraint = maxnorm(2),\n",
    "                        input_shape=(X_train.shape[1], X_train.shape[2], X_train.shape[3]),dim_ordering=\"th\")) \n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# dropout to reduce overfitting:\n",
    "model.add(Dropout(0.25))                 #### promijenio si na 0.\n",
    "\n",
    "# we use standard max pooling (halving the output of the previous layer):\n",
    "model.add(MaxPooling2D(pool_size=(3, 4), strides=(1, 3)))\n",
    "\n",
    "# the second convolution layer is 1x3\n",
    "model.add(Convolution2D(nb_filter, \n",
    "                        nb_row=1,\n",
    "                        nb_col=3,\n",
    "                        init='uniform', \n",
    "                        W_regularizer=l2(0.0001), \n",
    "                        W_constraint=maxnorm(2),dim_ordering='th'))   ###########dodao si dim_ordering='th'\n",
    "\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# we use max pooling again:\n",
    "model.add(MaxPooling2D(pool_size=(1, 3), strides=(1, 3)))\n",
    "\n",
    "# We flatten the output of the conv layer,\n",
    "# so that we can add a vanilla dense layer:\n",
    "model.add(Flatten())\n",
    "\n",
    "# we add two hidden layers:\n",
    "# increasing number of hidden layers may increase the accuracy, current number is designed for the competition \n",
    "model.add(Dense(hidden_dims, \n",
    "                init='uniform', \n",
    "                W_regularizer=l2(0.0001), \n",
    "                W_constraint = maxnorm(2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(hidden_dims, \n",
    "                init='uniform', \n",
    "                W_regularizer=l2(0.0001), \n",
    "                W_constraint = maxnorm(2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# We project onto a binary output layer to determine the category (Currently: normal/abnormal, but you can try train on the exact abnormality also)\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=loss, optimizer=optimizer,metrics=['accuracy'])\n",
    "\n",
    "# Saving model to Json (its easier to test it this way)    \n",
    "json_string = model.to_json()\n",
    "open(model_json, 'w').write(json_string)\n",
    "\n",
    "# Each time the loss will drop it will save weights file\n",
    "checkpointer = ModelCheckpoint(filepath=weights, verbose=1, save_best_only=True)\n",
    "\n",
    "# Start training\n",
    "history=model.fit(X_train, Y_train, batch_size=batch_size,\n",
    "          nb_epoch=nb_epoch,\n",
    "          shuffle=True,\n",
    "          callbacks=[checkpointer],\n",
    "          validation_data=(X_test, Y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testiranje modela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall\n",
      "0.8\n",
      "Specificity\n",
      "0.9285714285714286\n",
      "Mean score\n",
      "0.8642857142857143\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "maxlen = 100\n",
    "batch_size = 32\n",
    "loss = 'categorical_crossentropy'\n",
    "model_json='hb_model_orthogonal_experiment_norm.json'\n",
    "weights='Model2_weights.hdf5'\n",
    "\n",
    "def test(filename, model_json='hb_model_orthogonal_experiment_norm.json', \n",
    "         weights='Model1_weights.hdf5', \n",
    "         optimizer='adam', \n",
    "         loss='categorical_crossentropy'):\n",
    "   \n",
    "    #loads filename, transfrorms data using short-time Fourier transform, logscales the result and splits it into 129x129 squares\n",
    "    X = dataimport.data_from_file(filename=str(filename)+\".wav\", width=129, height=256, max_frames=10)\n",
    "    \n",
    "    predictions = np.zeros(len(X))\n",
    "    z = 0\n",
    "    \n",
    "    #Makes predictions for each 129x129 square\n",
    "    for frame in X:\n",
    "        predict_frame = np.zeros((1, 3, 129, 129))\n",
    "        predict_frame[0] = frame\n",
    "        predictions_all = model.predict_proba(predict_frame, batch_size=batch_size)\n",
    "        predictions[z] = predictions_all[0][1]\n",
    "\n",
    "        z += 1\n",
    "    \n",
    "    #Averages the results of per-frame predictions    \n",
    "    average = np.average(predictions)\n",
    "    average_prediction = round(average)\n",
    "    \t\t\n",
    "    #Prints the result\n",
    "  #  if int(average_prediction) == 0.0:\n",
    "        #append file with -1\n",
    "       # write_answer(filename=filename, result=\"-1\")\n",
    "        \n",
    "      #  print('Result for '+filename+': '+'Normal (-1)')\n",
    "        \n",
    "   # else:\n",
    "        #append file with 1\n",
    "       # write_answer(filename=filename, result=\"1\")    \n",
    "        \n",
    "     #   print('Result for '+filename+': '+'Abnormal (1)')        \n",
    "    \n",
    "    return int(average_prediction)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "os.chdir(\"C:\\\\Users\\\\Josip\")\n",
    "\n",
    "model = model_from_json(open(model_json).read())\n",
    "\n",
    "model.load_weights(weights)\n",
    "\n",
    "optimizer='adam'\n",
    "\n",
    "loss='categorical_crossentropy'\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "abnormal1=[]\n",
    "normal1=[]\n",
    "os.chdir(\"C:\\\\Users\\\\Josip\\\\new_test\\\\abnormal\\\\\")\n",
    "for filename in glob.glob('*.wav'):\n",
    "    abnormal1.append(test(filename[:-4], model_json='hb_model_orthogonal_experiment_norm.json', \n",
    "         weights='Model2_weights.hdf5', \n",
    "         optimizer='adam', \n",
    "         loss='categorical_crossentropy'))\n",
    "\n",
    "\n",
    "os.chdir(\"C:\\\\Users\\\\Josip\\\\new_test\\\\normal\\\\\")\n",
    "for filename in glob.glob('*.wav'):\n",
    "    normal1.append(test(filename[:-4], model_json='hb_model_orthogonal_experiment_norm.json', \n",
    "         weights='Model2_weights.hdf5', \n",
    "         optimizer='adam', \n",
    "         loss='categorical_crossentropy'))\n",
    "\n",
    "print(\"Recall\")\n",
    "print(sum(abnormal1)/len(abnormal1))\n",
    "\n",
    "print(\"Specificity\")\n",
    "print(1-sum(normal1)/len(normal1))\n",
    "\n",
    "print(\"Mean score\")\n",
    "print( (sum(abnormal1)/len(abnormal1)+1-sum(normal1)/len(normal1))  /2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
